
%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex
\chapter{Conclusion and future work}
\label{ch:conclusion}

The primary contribution of this dissertation consists of three parts: structured optimization, federated optimization, and applications of structured optimization in federated learning, where the key innovation is the usage and extension of modern duality theory. We review some of the results from this thesis and propose potential next paths in this chapter.


\section{Structured optimization}

In \autoref{ch:Dual-Struc-Opt}, we developed the polar alignment property characterizing the duality correspondence in structured optimization. As shown in \autoref{ch:App-Sig-Demix}, \autoref{ch:App-Primal-Retrieval} and \autoref{ch:App-AtomicOpt}, the polar alignment property allows us to design new models and develop efficient algorithms for structured optimization. Further research opportunities remain. For example, most (if not all) of the
ideas we have presented could be generalized to the infinite-dimensional
setting, which would accommodate more general decompositions. Also, other
standard algorithms, such as splitting and bundle methods~\cite{fan2019bundle}, seem to exhibit properties that can easily be explained using the language of polar alignment.

In \autoref{ch:App-Sig-Demix}, we develop a two-stage approach for deconvolving a superposition of mutiple structural signals into its constituent components. There are opportunities for future research. As we have demonstrated, the random rotation model is a useful mechanism for introducing incoherence among the individual signals. However, even in contexts where it's possible to rotate the signals, it may prove too costly to do so in practice because the rotations need to be applied at each iteration of the algorithm in \autoref{ch:App-AtomicOpt}. We might then consider other mechanisms for introducing incoherence that are computationally cheaper, and rely instead, for example, on some fast random transform. The literature on demixing abounds with various incoherence notions. We wish to explore what is the relationship between these and definition of $\beta$-incoherence that we adopt. Alternative incoherence definitions may prove useful in deriving other mechanisms for inducing incoherence in the signals. Moreover, a significant assumption of our analysis is that the parameters $\lambda_i$ exactly equilibrate the gauge values for each signal. In practice, there are many cases where the parameters $\lambda_i$ are known. For example, in some secure communication problems~\citep[Section~1.3.1]{mccoy2014sharp}, the sender can normalize the signals before mixing them. On the other hand, when the parameters $\lambda_i$ are not known, we can apply a grid search to obtain reasonable approximations. It may be possible to analyze how the stability in the recovery of the signals depends on errors that might exist in the ideal parameter choices.

In \autoref{ch:App-Primal-Retrieval}, we develop a primal-retrieval strategy for structured optimization. Further research opportunities remain, particularly for designing meaningful primal-retrieval strategy for non-polyhedral and non-spectral atomic sets. Moreover, the primal-retrieval technique we developed is algorithm-agnostic, it is possible to develop more efficient primal-retrieval rule that associated with specific optimization algorithms such as the conditional-gradient method.

In \autoref{ch:App-AtomicOpt}, we introduce our open-source Julia package \texttt{AtomicOpt.jl} for solving structured optimization problems. There are at least two possible opportunities to improve this package: adding more dual-based candidate algorithms besides the ones introduced in \autoref{sec:5-2}, and adding more basic atomic sets besides the ones introduced in \autoref{sec:5-3}. 

\section{Federated optimization}

In \autoref{ch:Dual-Struc-Opt}, by tackling the dual problem of federated optimization, we propose the federated dual coordinate descent (FedDCD) algorithm and its variants. More importantly, FedDCD provides a general framework for federated optimization and suggests many interesting future research directions. First, it is possible to develop an asynchronous version of FedDCD by leveraging well-studied analysis of asynchronous parallel coordinate descent methods~\citep{LiuW15,0002WRBS15}. Next, one might consider client sampling strategies other than the standard uniform sampling. For example, there are some recent studies of the coordinate descent with the greedy selection rule~\citep{NutiniSLFK15,BCD_julie,fang2020greed}, which can be adopted with FedDCD. Finally, the lower bound of complexity of first-order methods with random participation for federated optimization is still an open problem. As we have shown in \autoref{sec:lowerBound}, our lower bound analysis has a $\BigOh(\sqrt{N})$ gap to the upper bound of accelerated FedDCD. We hope to explore whether the lower bound can be further tightened or an algorithm with a faster convergence rate can be developed. 


\section{Applications of structured optimization in federated learning}

In \autoref{ch:Val-HFL} and \autoref{ch:Val-VFL}, we develop contribution valuation strategies ComFedSv and VerFedSV, respectively for, horizontal federated learning and vertical federated learning.  Our study advances the frontier in this emerging important direction. There are also many interesting future directions. For example, under the framework of federated learning, there may exist some other properties needed for fairness in federated learning in addition to those in the Shapley fairness. As another example, we otice that when we keep adding clients with identical features, the total of the VerFedSV increases. This may suggest that some clients may ``cheat'' by constructing new clients with identical features so that they can receive unjustifiable rewards in the end. The issue can be resolved under the synchronous setting, for example, by checking the similarity between uploaded embeddings from clients. However, it seems there is no simple solution to resolve this issue under the asynchronous setting. 
