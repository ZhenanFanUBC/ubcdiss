%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex
\chapter{Primal-retrieval strategy for structured optimization}
\label{ch:App-Primal-Retrieval}

\section{Introduction} \label{sec:4-1}

Consider the problem of fitting a model to data by selecting model parameters as the superposition of a limited set atomic elements taken from a given dictionary. Versions of this cardinality-constrained problem appear in a number of statistical-learning applications in machine learning \cite{tibshirani1996regression,yul06,Meinshausen06,aep08}, data mining, and signal processing~\cite{candes:2013}. In these applications, common atomic dictionaries include the set of one-hot vectors or matrices, rank-1 unit matrices, usually chosen as a mechanism to encode a notion of parsimony in the definition of the model parameters.

The formulation we consider takes the form
\begin{equation} \label{eq:main_prob} 
    \find x\in\Re^n \sut f(b - Mx) \leq \alpha \tand \card_\Ascr(x) \leq k,
\end{equation}
where $f:\Re^n\to\Re$ is an $L$-smooth and convex function, $M: \Re^n \to \Re^m$ is a linear operator with $m < n$, $b \in \Re^m$ is the observation vector, and $\Ascr \subseteq \Re^n$ is the atomic set. The cardinality function 
\begin{equation}\label{eq-card-fcn}
    \card_\Ascr(x) \coloneqq \inf\left\{ \mathbf{nnz}(c) ~\bigg\vert~ x = \textstyle\sum\limits_{a \in \Ascr} c_a a, \enspace c_a \geq 0\right\}
\end{equation}
measures the complexity of $x$ with respect to the atomic set $\Ascr$.  The loss term $f(b - Mx)$ measures the quality of the fit. When $\Ascr = \{ \pm e_1,\pm e_2, \ldots,\pm e_n \}$ is the set of signed canonical unit vectors, the function $\card_\Ascr(x)$ simply counts the number of nonzero elements in $x$. Typically $k\ll n$, which indicates that we seek a feasible model parameter $x$ that has an efficient representation in terms of $k$ atoms from the set $\Ascr$.

For the application areas that we hope to target, the two characteristics of this feasibility problem that pose the biggest challenge to efficient implementation are the combinatorial nature of the cardinality constraint, and the high-dimensionality of the parameter space. To address the combinatorial challenge, we build on the general atomic-sparsity framework introduced in \autoref{sec:1-3} and use the convex gauge function as a tractable proxy for the cardinality function~\eqref{eq-card-fcn}; see \eqref{eq:gauge1}. In tandem with the convexity of the loss function, this function allows us to formulate three alternative relaxed convex optimization problems that under certain conditions have approximate solutions that satisfy the feasibility problem; see problems~\eqref{prob:primal1},~\eqref{prob:primal2}, and~\eqref{prob:primal3} in \autoref{sec:4-2}.

The high-dimensionality of the parameter space, however, may imply that it's inefficient---and maybe even practically impossible---to solve these convex relaxations because it's infeasible to directly store the approximations to a feasible solution $x$. Instead, we wish to develop methods that leverage the efficient representation that feasible solutions have in terms of the atoms in the dictionary $\Ascr$. For example, consider the case in which the dictionary is the set of ${n \times n}$ rank-one matrices, and $M$ is the trace linear operator that maps these matrices into $m$-vectors. Any method that iterates directly on the parameters $x$ requires $\BigOh(n^2+m)$ storage. An alternative is the widely-used conditional gradient method \cite{frank1956algorithm}, which requires $\BigOh( n t+m )$ storage after $t$ iterations~\cite{jaggi2013revisiting}, but also often requires a substantial number of iterations $t$ to converge. Instead of storing $x$ directly, we use the dual problems to the convex relaxations ~\eqref{prob:primal1},~\eqref{prob:primal2}, and~\eqref{prob:primal3}, which require only $\BigOh(m)$ storage, and still allow us to collect information on which atoms in $\Ascr$ participate in the construction of a feasible $x$.

\subsection{Approach}

We propose a unified algorithm-agnostic strategy that uses any sequence of improving dual solutions to one of the convex relaxations to identify an essential subset of atoms in $\Ascr$ needed to construct an $\epsilon$-infeasible solution $x$ that satisfies
\begin{equation} \label{eq:feasible}
    f(b - Mx) \leq \alpha + \epsilon \tand \card(\Ascr; x) \leq k
\end{equation}
for any positive tolerance $\epsilon$. These \emph{atomic-identification} rules, described in \autoref{sec:atom_iden}, derive from the polar-alignment property and apply to arbitrary dictionaries $\Ascr$ \cite{fan2019alignment}. These atom-identification rules generalize earlier approaches described by El~Ghaoui~\cite{Ghaoui12} and Hare and Lewis~\cite{hare2004identifying}. Once an essential subset of $k$ atoms is identified, an $\epsilon$-feasible solution $x$ can be computed by optimizing over all positive linear combinations of this subset. This relatively small $k$-variable problem can often be solved efficiently.

We prove that when the atomic dictionary is polyhedral, we can set $\epsilon$ to zero and still identify in polynomial time a set of feasible atoms; see \autoref{coro:polyhedral}. When the atomic dictionary is spectrahedral, we prove that an $\epsilon$-feasible set of atoms can be identified also in polynomial time; see \autoref{coro:spectral}. 

We demonstrate via numerical experiments on real-world datasets that this approach is effective in practice.

There are three important elements in our primal-retrieval algorithm. The first element is an atom-identifier function $\EssCone_{\Ascr, k}(\cdot)$ that maps $M^*y$, where $y$ is any feasible dual variable, to a cone generated by $k$ atoms that have been identified as \emph{essential}, that is, these atoms have the property that 
\[
  \EssCone_{\Ascr, k}(M^*y) \subseteq \{x \mid \card(A; x) \leq k\}.
\]
The explicit definition of the essential cone depends on the particular atomic set $\Ascr$. In \autoref{sec:primal-retrieval}, we make it explicit for atomic sets that are polyhedral (\autoref{sec:polyhedral}) and spectral (\autoref{sec:spectral}).

The second element is an arbitrary function $\texttt{oracle}_{f,\Ascr,M,b}(\cdot)$ (such as an appropriate first-order iterative method) that generates dual iterates $y^{(t)}$ converging to the optimal dual variable $y^*$ of any of the dual problem \Drobi. It's this oracle that generates the dual estiamtes subsequently used by $\EssCone_{\Ascr,k}$.

The third algorithmic component is the reduced convex optimization problem
\begin{equation} \label{eq:primal_recover} \tag{PR}
    x^{(t)} \in \argmin{x}\{ f(b - Mx) | x \in \EssCone_{\Ascr, k}(M^*y^{(t)})\},
\end{equation}
which at each iteration constructs a primal estimate $x^{(t)}$ using the atoms identified through the dual estimate $y^{(t)}$.  The detailed algorithm is shown in Algorithm~\ref{alg:primal_recover}. Note that our primal-retrieval strategy is not aiming to recover the optimal solutions to~\eqref{prob:primal1}, \eqref{prob:primal2} or~\eqref{prob:primal3}. These problems serve as a guidance for our atom-identification rule. The final output of Algorithm~\ref{alg:primal_recover} maybe different from the optimal solutions to~\eqref{prob:primal1}, \eqref{prob:primal2} or~\eqref{prob:primal3}.

\begin{algorithm}[t]
    \DontPrintSemicolon\setcounter{AlgoLine}{-1}
    \SetKwComment{tcp}{[}{]}
    \KwIn{data-fitting constraint $\alpha$, cardinality constraint $k$, atomic set $\Ascr$, loss function $f$, linear operator $M$, observation $b$ and tolerance $\epsilon\geq0$} 
    \For{$t = 1, 2, \dots$}{
        $y^{(t)} \leftarrow \texttt{oracle}_{f,\Ascr,M,b}(y^{(t-1)})$\;
        $x^{(t)} \leftarrow$ solution to~\eqref{eq:primal_recover}\;
        \lIf{$f(b - Mx^{(t)}) \leq \alpha + \epsilon$}{break}
    }
    \Return{$x^{(t)}$}
    \caption{Primal-retrieval algorithm} 
    \label{alg:primal_recover}
\end{algorithm}


\subsection{Related work}

Indeed, many recently proposed algorithms for structured optimization problems are dual-based algorithm~\cite{fan2019bundle,DingYCTU21}.  However, we need to retrieve the primal variable $x$ at some point when solving the dual problem, and this can again explode the memory. A widely used heuristic applies truncated SVD \cite[Algorithm~6.4]{fan2019alignment} to obtain a atomic-sparse representation of the solution, but this heuristic lacks reliability since its recovery error bound is not well studied. How to efficiently and reliably retrieve a atomic-sparse primal variable is crucial for memory-efficient structured optimization.

Solving dual formulations of nuclear-norm or trace-norm regularized problems has received substantial interest in recent years since the dual-based approaches are shown to enjoy ``optimal storage'', i.e., have space complexity $\BigOh(m)$ instead of $\BigOh( n^2 )$ \cite{DingYCTU21}. For example, the bundle method for solving the Lagrangian dual formulation of semi-definite programming \cite{helmberg2000spectral} and the gauge dual formulation of general atomic sparse optimization problem \cite{fan2019bundle} are shown to achieve promising empirical result.

Another related work line is directly developing a memory-efficient primal-based algorithm based on hard-thresholding. For example, the gradient hard thresholding algorithm \cite{YuanLZ17}, the periodical hard-thresholding \cite{Allen-ZhuHHL17} and many proximal-gradient or ADMM-based algorithms that use hard-thresholding as an implementation heuristic \cite{mazumder2010spectral,Lin11,hsieh2014nuclear}. This line of works are primal-based algorithms and are tangential to this paper. We do not include them in our discussion. 

The theoretical analysis of our primal-retrieval is related to atom-identification \cite{BurM88,hare2004identifying,hare2011identifying}, and especially some recently developed safe-screening rules \cite{Ghaoui12,wang2013lasso,liu2014safe,WangZLWY14,Raj2015ScreeningRF,BonnefoyERG15,XiangWR17,NdiayeFGS17,ZhangHLYCHW17,kuang2017screening,Atamtrk2020SafeSR,Bao20} for various kind of sparse optimization problems. Our \autoref{thm:p0} can be viewed as a generalization of the gap-safe screening rules to general structured optimization problems as well as more problem formulations. Some of the techniques used in our analysis are related to the facial reduction strategy from Krislock and Wolcowicz~\cite{krislock2010explicit}. 

\subsection{Summary of contributions}

In this chapter, we proposed a simple primal-retrieval strategy for a class of structured data-fitting problems using the polar alignment property developed in \autoref{ch:Dual-Struc-Opt}. Given a dual-based algorithm converging to the optimum dual solution, we demonstrate theoretically that for polyhedral atomic sets, our proposed strategy can obtain a feasible primal variable, and for spectral atomic sets, our proposed strategy can obtain a near-feasible primal variable. Extensive numerical experiments are conducted to support our analysis. 


\section{Convex relaxations}
\label{sec:4-2}

In this section, we introduce convex relaxations to the structured data-fitting problem~\eqref{eq:main_prob}. In particular, 
we consider the following three related gauge-regularized optimization
problems:
\begin{align} 
  & \minimize{x}\enspace p_1(x) := f(b - Mx) + \lambda\gauge\As(x), \label{prob:primal1} \tag{P$_1$} \\
  & \minimize{x}\enspace p_2(x) := f(b - Mx) \enspace\text{subject to}\enspace \gauge\As(x) \leq \tau, \label{prob:primal2} \tag{P$_2$} \\
  & \minimize{x}\enspace p_3(x) := \gauge\As(x)  \enspace\text{subject to}\enspace f(b - Mx) \leq \alpha \label{prob:primal3} \tag{P$_3$}. 
\end{align}
It's well known that under mild conditions, these three formulations are equivalent for appropriate choices of the positive parameters $\lambda, \tau$, and $\alpha$~\cite{FrieTsen:2006}. Practitioners often prefer one of these formulations depending on their application. For example, tasks related to machine learning, including feature selection and recommender systems, typically feature one of the first two formulations~\cite{tibshirani1996regression,yul06,Meinshausen06}.  On the other hand, applications in signal processing and related fields, such as as compressed sensing and phase retrieval, often use the third formulation~\cite{berg2008probing,candes:2013}. 

Our primal-retrieval strategy relies on the hypothesis that the atomic-sparse optimization problems \eqref{prob:primal1}, \eqref{prob:primal2} and \eqref{prob:primal3} are reasonable convex relaxations to the structured data-fitting problem~\eqref{eq:main_prob}, in the sense that the corresponding optimal solutions are feasible for \eqref{eq:main_prob}. We formalize this in the following assumption.

\begin{assumption} \label{ass:blanket}
  Let $x^*$ denote the optimal solution to any of \eqref{prob:primal1}, \eqref{prob:primal2} or \eqref{prob:primal3}. Then $x^*$ is feasible for \eqref{eq:main_prob}: 
  \[f(b - Mx^*) \leq \alpha \tand |\suppa(x^*)| \leq k.\]
\end{assumption}

As described in \autoref{sec:4-1}, the Fenchel-Rockafellar duals for these problems have typically smaller space complexity. These dual problems can be formulated as
\begin{align} 
  & \minimize{y}  d_1(y) := f^*(y) - \ip{b}{y} \text{subject to} \sigma\As(M^*y) \leq \lambda, \label{prob:dual1} \tag{D$_1$}\\
  & \minimize{y} d_2(y) := f^*(y) - \ip{b}{y} + \tau\sigma\As(M^*y), \label{prob:dual2} \tag{D$_2$}\\
  & \minimize{y} \inf_{\beta > 0} \enspace d_3(y, \beta)
    := \beta \left(
      f^*\left( y/\beta \right) + \alpha
    \right) - \ip{b}{y} \label{prob:dual3} \tag{D$_3$}\\
  & \mbox{subject to}\enspace \sigma\As(M^* y) \leq 1. \nonumber
\end{align}
The derivation of these dual problems can be found in \autoref{app:duals}. 

\section{Atom identification} \label{sec:atom_iden}

In this section, we demonstrate how the optimal dual variables can be used to identify essential atoms that participate in the primal solutions. In order to develop atomic-identification rules that apply to arbitrary atomic set $\Ascr\subseteq\Re^n$---even those that are uncountably infinite---we require generalized notions of active constraint sets. In linear programming, for example, the simplex multipliers give information about the optimal primal support. By analogy, our atomic-identification rules give information about the essential atoms that participate in the support of the primal optimal solutions. In addition, we extend the identification rules to approximate the essential atoms from approximate dual solutions. 

We build on the following
result, due to Fan et al.~\cite[Proposition~4.5 and Theorem~5.1]{fan2019alignment}.
\begin{theorem}[Atom identification]\label{thm:opt_supp_id} Let
  $x^*$ and $ y^*$ be optimal primal-dual solutions for problems \Probi~and \Drobi, with
  $i=1,2,3$. Then
  \[\suppa(x^*) \subseteq \Escr\As(M^*y^*). \]
\end{theorem}

The following theorem generalizes this result to show similar atomic support identification properties that also apply to approximate dual solutions. In particular, given a feasible dual variable $y$ close to $y^*$, the support of $x^*$ is contained in the set of $\epsilon$-exposed atoms, which is defined by
\begin{equation}
  \Escr\As(z, \epsilon) \coloneqq \{a\in\Ascr \mid \ip{a}{z} \geq \sigma\As(z) - \epsilon\}.
\end{equation}
Obviously, for any $\epsilon \geq 0$, we would have 
\[\Escr\As(z) \subseteq \Escr\As(z, \epsilon).\]

\begin{theorem}[Generalized atom identification]\label{thm:p0}
  Let $x_i$ and $y_i$ be feasible primal and dual vectors, respectively for
  problems \Probi\ and \Drobi, with $i = 1, 2, 3$. Then
  \begin{equation}\label{eq:1} 
    \suppa(x_i^*) \subseteq \Escr\As(M^*y_i, \epsilon_i),
  \end{equation}
  where each $\epsilon_i$ is defined for problem $i$ by
  \begin{itemize} \setlength\itemsep{-0.0em}
    % \item \label{thm:p1} $\epsilon_1 = \opa{M} \| y_1 - y_1^* \| $,
    % \item \label{thm:p2p3} $\epsilon_i = 2\opa{M} \| y_i - y_i^* \|$,~\quad $i \in \{2,3\}$. \blu{[Change to $d(y) - d(y^*)$]}
  \item \label{thm:p1} $\epsilon_1 = \opa{M}\sqrt{2L \left( d_1(y_1) - d_1(y_1^*) \right)}$,

  \item \label{thm:p2} $\epsilon_2 = 2\opa{M}\sqrt{2L \left( d_2(y_2) - d_2(y_2^*) \right)}$,
    
  \item \label{thm:p3} $\epsilon_3 = 2\opa{M}\sqrt{2\bar{\beta}L (
        \max\{d_3(y_3, \underline{\beta}), d_3(y_3, \bar{\beta})\} - d_3( y_3^*, \beta^*) ) }$, 
  \end{itemize}
  where $\underline{\beta}$ and $\bar{\beta}$ are positive lower and
  upper bounds, respectively, for $\beta^*$, and $\|M\|\As:=\max_{a\in\Ascr}\|Ma\|_2$ is the induced atomic operator norm.
\end{theorem}

\autoref{thm:p0} asserts that the underlying atomic support of $x_i^*$ is contained in the set of the $\epsilon$-exposed atoms of $M^* y_i$.  Moreover, when $y_i\to y_i^*$ (and, for problem~\eqref{prob:dual3}, the bounds $\underline\beta\to\beta^*$ and $\bar\beta\to\beta^*$), each $\epsilon_i\to0$, and thus~\eqref{eq:1} implies that we have a tighter containment for the optimal atomic support.  The proofs for parts (a) and (b) of \autoref{thm:p0} depend on the strong convexity of $f^*$, which is implied by the Lipschitz smoothness of $f$. This convenient property, however, is not available for part (c) because the dual objective of \eqref{prob:dual3} is the perspective map of $f^* + \alpha$, which is not strongly convex~\cite{aravkin2018foundations}. We resolve this technical difficulty by instead imposing the additional assumption that bounds are available on the dual optimal variable $\beta^*$. \autoref{app:bounds} describes how to obtain these bounds during the course of the level-set method developed by Aravkin et al.~\cite{aravkin2016levelset}. 

\section{Primal retrieval} \label{sec:primal-retrieval}

\autoref{thm:p0} serves mainly as a technical tool for error bound analysis, in particular because it's impractical to compute or approximate $\epsilon_i$. However, \autoref{thm:opt_supp_id} and \autoref{thm:p0} motivate us to define an atom-identifier function $\EssCone_{\Ascr, M, k}$ that depends on the dual variable $y$ and satisfies the inclusions 
\[\cone(\Escr\As(M^*y)) \subseteq \EssCone_{\Ascr, k}(M^*y) \subseteq \cone(\Escr\As(M^*y, \epsilon)).\]

The next two sections demonstrate how to construct such a function for polyhedral and spectral atomic sets, which are two important examples that appear frequently in practice. With this function we can thus implement the primal-recovery problem required by Step~3 of Algorithm~\ref{alg:primal_recover}. Moreover, we show how to use the error bounds of \autoref{thm:p0} to analyze the atomic-identification properties of the resulting algorithm.


\subsection{Primal-retrieval for polyhedral atomic sets} \label{sec:polyhedral}

In this section we formalize a definition for the function $\EssCone_{\Ascr, k}$ for the case in which $\Ascr$ is a finite set of vectors, which implies that the convex hull is  polyhedral. Given a feasible dual vector $y$, consider the top-$k$ atoms in $\Ascr$ with respect to the inner product with the vector $M^*y$:
\begin{equation} \label{eq:top-k}
    \Ascr_k := \{a_1, \dots, a_k\} \subseteq \Ascr \text{such that} \ip{M^*y}{a_i} \geq \ip{M^*y}{a} \enspace\forall i \in [k] ~\mbox{and}~ a \in \Ascr\setminus\{a_1, \dots, a_k\}.
\end{equation}
Note that there may be many sets of $k$ atoms that satisfy this property. We then construct the cone of essential atoms as the convex conic hull generated from this set of top-$k$ atoms, i.e.,
\begin{equation}\label{eq-essential-cone-poly}
  \EssCone_{\Ascr, k}(M^*y) \coloneqq \cone\Ascr_k.
\end{equation}
Thus, the primal-retrieval computation in step~3 of Algorithm~\ref{alg:primal_recover} is given by 
\begin{equation*}
\hat x = \sum_{i=1}^k \hat c_ia_i, 
\end{equation*}
where
\begin{equation} \label{eq-top-k-recover}
  \hat c \in \argmin{c \in \Re^k_+} f_k(c),
  \text{with} f_k(c) := f\left( b - M\sum_{i=1}^k c_ia_i \right),
\end{equation}
is the $k$-vector of coefficients obtained by minimizing the reduced objective over a $k$-dimensional polyhedron defined by the top-k atoms.

\begin{example}[Sparse vector recovery] \label{ex:bpdn}
  We consider the problem of recovering a sparse vector $x^\natural$ from noisy observations $b:=Mx^\natural + \eta$, where $M:\Re^n \to \Re^m$ is a given measurement matrix and $\eta \in \Re^m$ is standardized Gaussian noise. For some expected noise level $\alpha > 0$, the sparse recovery problem can be thus be expressed as 
  \begin{equation} \label{eq:sparse_recovery}
      \find x\in\Re^n \sut \|b - Mx\|_2 \leq \alpha \tand \texttt{nnz}(x) \leq k,
  \end{equation}
  which corresponds to~\eqref{eq:main_prob} with $f=\|\cdot\|_2$ and with the atomic set
  \begin{equation}\label{eq-signed-e}
    \Ascr = \{\pm e_1, \dots, \pm e_n\},
  \end{equation}
  where each $e_i$ is the $i$th canonical unit vector. The basis pursuit denoising (BPDN) approach approximates this problem by replacing the cardinality constraint with an optimization problem that minimizes the 1-norm of the solution:
  \begin{equation} \label{eq:bpdn}
    \minimize{x\in\Re^n} \|x\|_1  \enspace\mbox{subject to}\enspace \|Mx - b\|_2 \leq \alpha;
  \end{equation}
  see Chen et al.~\cite{cds98}. This convex relaxation corresponds to problem \eqref{prob:primal3}.
  
  There are many dual methods that generate iterates $y^{(t)}$ converging to the optimal dual solution to~\eqref{eq:bpdn}, including the level-set method coupled with the dual conditional-gradient suboracle, as described by~\cite{aravkin2016levelset,fan2019alignment}. The resulting primal-retrieval strategy for Step~3 of Algorithm~\ref{alg:primal_recover} can thus be implemented by executing the following steps:
  \begin{itemize}
      \item (Top-$k$ atoms) Find the top $k$ indices $\{i_1, \dots, i_k\} \subset [n]$ of the vector $M^*y^{(t)}$ with largest absolute value and gather their corresponding signs $s_i \coloneqq \sign([M^*y^{(t)}]_i)$ for $i \in \{i_1, \dots, i_k\}$. The top-$k$ atoms are thus $\Ascr_k=\{s_{i_1}e_{i_1},\ldots,s_{i_k}e_{i_k}\}$; see~\eqref{eq:top-k}.
      \item (Retrieve coefficients) Solve the reduced problem~\eqref{eq-top-k-recover}, where in this case,
      \begin{equation} \label{eq:bpdn_pr}
          c^{(t)} \in \argmin{c \in \Re_+^k} f_k(c),
          \text{where} f_k(c) = \|M[s_{i_1}e_{i_1} \cdots s_{i_k}e_{i_k}]c - b\|_2.
      \end{equation}
      This is a nonnegative least-squares problem for which many standard algorithms are available. For example, an accelerated projected gradient descent method requires $\BigOh( m k \log(1/\epsilon) )$ iterations when $M[s_{i_1}e_{i_1} \dots s_{i_k}e_{ik}]$ has full column rank. 
      \item (Termination) Step~4 of Algorithm~\ref{alg:primal_recover} is implemented simply by verifying that $f_k(c^{(t)}) \leq \alpha$. (As verified by \autoref{coro:polyhedral}, we may take $\epsilon=0$ in this polyhedral case.) Thus, we can terminate the algorithm and return the primal variable 
      \[x^{(t)} = [s_{i_1}e_{i_1} \dots s_{i_k}e_{ik}]c^{(t)},\]
      which is the superposition of the top-$k$ atoms.  Otherwise, the algorithm proceeds to the next iteration. 
  \end{itemize}
  We describe numerical experiments for the sparse vector recovery problem in \autoref{sec:bpdn}. 
\end{example}

\subsubsection{Iteration complexity}\label{sec-iteration-complexity}

In order to guarantee the quality of the recovered solution, we rely on a notion of degeneracy introduced by Nutini et al.~\cite{nutini2019active}. 

\begin{definition}
  Let $x^*$ and $y^*$, respectively, be optimal primal and dual solutions for problems \Probi\ and \Drobi, where $\Ascr$ is polyhedral. Let $\delta$ be a positive scalar. The problem pair (\Probi, \Drobi) is $\delta$-nondegenerate if for any $a\in\Ascr$, either $a\in\suppa(x^*)$ or $\ip{a}{M^*y^*} \leq \sigma\As(M^*y^*) - \delta$. 
\end{definition}

The next proposition guarantees a finite-time atom identification property when the atomic set is polyhedral.

\begin{proposition}[Finite-time atom-identification] \label{prop:polyhedral} For each problem $i
  = 1, 2, 3$, let $\{y_i^{(t)}\}_{t=1}^\infty$ be a sequence that converges to an optimal dual solution $y_i^*$. If the atomic set $\Ascr$ is polyhedral and the problem pair (\Probi, \Drobi) is $\delta$-nondegenerate, then there exists $T > 0$ such that 
  \[
   \EssCone_{\Ascr, k}(M^*y^{(t)}) \supseteq \Escr\As( M^* y_i^*) 
  \text{and}
  x^{(t)} \mbox{ is feasible for } \eqref{eq:main_prob} 
  \quad \forall t>T.\]
  It follows that Algorithm~\ref{alg:primal_recover} will terminate in $T$ iterations regardless of the tolerance $\epsilon$. 
\end{proposition}

\autoref{prop:polyhedral} ensures that the atom-identification property described by~\autoref{thm:p0} is guaranteed to discard superfluous atoms in a finite number of iterations as long as we have available an iterative solver that generates dual iterates converging to a solution. Thus, Algorithm~\ref{alg:primal_recover} is guaranteed to generate a feasible solution to~\eqref{eq:main_prob}.
The following corollary characterizes a bound on $T$ in terms of the convergence rate of the dual method.

\begin{corollary} \label{coro:polyhedral}
  For each problem $i= 1, 2, 3$, suppose the dual oracle generates iterates $y^{(t)}$ converging to optimal variable $y_i^*$ with rate
  \[d_i(y_i^{(t)}) - d_i(y_i^*) \in \Oscr\left( t^{-\alpha} \right)\]
  for some $\alpha > 0$. If the atomic set $\Ascr$ is polyhedral and the problem pair (\Probi, \Drobi) is $\delta$-nondegenerate, then Algorithm~\ref{alg:primal_recover} with $\epsilon=0$ terminates in $\Oscr\left(\delta^{-2/\alpha}\right)$ iterations. 
\end{corollary}

\subsubsection{Centrosymmetry and unconstrained primal recovery}

Further computational savings are possible when the atomic set $\Ascr$ is centrosymmetric, i.e.,
\begin{equation}\label{eq-centro-symmetric}
  a\in\Ascr \iff -a\in\Ascr.
\end{equation}
Centrosymmetry is a common property, and perhaps the prototypical example is the set of signed canonical unit vectors given by the set~\eqref{eq-signed-e}. Whenever centrosymmetry holds, $\cone\Ascr=\Span\Ascr$. This motivates us to replace the function $\EssCone$ with the function
\[
  \EssSpan_{\Ascr, k}(M^*y) \coloneqq \Span\Ascr_k,
\]
where $\Ascr_k$ is the collection of top-$k$ atoms defined by \autoref{eq:top-k}. Thus, the primal-retrieval optimization problem~\eqref{eq-top-k-recover} reduces to the unconstrained version
\begin{equation} \label{eq:primal_recover_polyhedral2}
  \hat c \in \argmin{c \in \Re^k} f_k(c).
\end{equation}

The following corollary simply asserts that the complexity results described in \autoref{sec-iteration-complexity} continue to hold for centrosymmetric atomic sets when using the essential span function.
\begin{corollary}[Atom identification under centrosymmetry]
    If the atomic set $\Ascr$ is centrosymmetric and polyhedral, then \autoref{prop:polyhedral} and \autoref{coro:polyhedral} hold with $\EssCone$ replaced by $\EssSpan$. 
\end{corollary}


\subsection{Primal-retrieval for spectral atomic sets} \label{sec:spectral}

In this section we formalize a definition for the function $\EssCone_{\Ascr, k}$ for the case in which $\Ascr$ is a collection of rank-1 matrices, either asymmetric or symmetric, respectively:
\begin{align} \label{eq:asymmetric}
    \Ascr &= \{uv^T \mid u \in \mR^m,\ v \in \mR^n,\ \|u\|_2=\|v\|_2 = 1\}, \\ \label{eq:symmetric}
    \Ascr &= \{vv^T \mid v \in \mR^n,\ \|v\|_2 = 1\}.
\end{align}
We mainly focus on the former atomic set of asymmetric matrices because all of our theoretical results easily specialize to the symmetric case. Note that this atomic set is centrosymmetric (cf. \autoref{eq-centro-symmetric}), and as we'll see below, the recovery problem is unconstrained. Later in \autoref{sec:symmetric-matrices} we'll describe the recovery problem for the atomic set of symmetric matrices, which is in fact a non-centrosymmetric set.

For this section only, we work with the linear operator $M:\Re^{m\times n}\to\Re^{p\times q}$, and replace the vector of observations $b$ with the $p$-by-$q$ matrix $B$. In this context, the dual variables for one of the corresponding dual problems is a matrix of the same dimension.

Fix a feasible dual variable $Y$ and define the SVD for its product with the adjoint of $M$ by
\begin{equation}\label{eq-svd}
   M^*(Y) = 
\begin{bmatrix}
    U_k & U_{-k}
\end{bmatrix}
\begin{bmatrix}
    \Sigma_k &          \\ 
             &  \Sigma_{-k}
\end{bmatrix}
\begin{bmatrix}
    V_k^T \\ 
    V_{-k}^T
\end{bmatrix},
\end{equation}
where $\Sigma_k$ is the diagonal matrix consisting of top-$k$ singular values of $M^*(Y)$, the matrices $U_k$ and $V_k$ contain the corresponding left and right singular vectors, and the matrices $U_{-k}$, $V_{-k}$, and $\Sigma_{-k}$ contain the remaining singular vectors and values. Then the reduced atomic set by $U_k$ and $V_k$ can be expressed as 
\[
  \Ascr_k = \{ uv\T \mid u\in\range(U_k),\ v\in\range(V_k),\ \|u\|_2=\|v\|_2=1\} \subset \Ascr.
\]
We construct the cone of essential atoms as the convex cone generated from the reduced atomic set $\Ascr_k$, i.e.,
\begin{equation} \label{eq:cone_spectral}
    \EssCone_{\Ascr, k}(M^*(Y)) \coloneqq \cone(\Ascr_k) = \{U_k C V_k^T | C \in \Re^{k\times k}\}.
\end{equation}
Thus, the primal-retrieval computation in Step~3 of Algorithm~\ref{alg:primal_recover} is then given by 
\begin{equation}\label{eq:unsymm-recovered-soln} 
  \hat X = U_k\hat C V_k^T
\end{equation}
where
\begin{equation}\label{eq-spectral-reduced}
  \hat C \in \argmin{C\in\Re^{k\times k}} f_k(C), \text{with} f_k(C) \coloneqq f\left( B - M(U_k C V_k^T) \right),
\end{equation}
is a $k\times k$ matrix obtained by solving the reduced problem~\eqref{eq:primal_recover}, which is defined over the cone generated by the essential atoms identified through the top-$k$ singular triples of $M^*(Y)$, desribed by~\eqref{eq:cone_spectral}.


\begin{example}[Low-rank matrix completion] \label{ex:mc}
  The low-rank matrix completion (LRMC) problem aims to recover a low-rank matrix from partial observations, which arises in many real applications such as recommender systems~\cite{rennie2005fast} and in a convex formulation of the phase retrieval problem~\cite{candes2013phaselift}. The LRMC problem can be expressed as 
  \begin{equation} \label{eq:lrmc}
    \find X\in\Re^{m\times n} \sut \sum_{(i,j) \in \Omega}\tfrac{1}{2}\left(X_{i,j} - B_{i,j}\right)^2 \leq \alpha \tand \rank(X) \leq k,
  \end{equation}
  where $\{B_{i,j} \mid (i,j) \in \Omega\}$ is the set of observations over the index set $\Omega$. Problem~\eqref{eq:lrmc} corresponds to~\eqref{eq:main_prob} with the objective $f=\half\|\cdot\|_2^2$, the atomic set $\Ascr$ given by~\eqref{eq:asymmetric}, and the linear operator $M$ defined by the mask 
  \[ M(X)_{i,j} = 
  \begin{cases}
      X_{i,j} &(i,j) \in \Omega, \\
      0 &\mathrm{otherwise}.
  \end{cases}
  \]
  Fazel~\cite{fazel1998approximations} popularized the convex relaxation of~\eqref{eq:lrmc} that minimizes the sum of singular values of $X$:
  \begin{equation} \label{eq:mc}
  \minimize{X\in\Re^{m\times n}}\enspace \|X\|_*  \enspace\text{subject to}\enspace \sum_{(i,j) \in \Omega}\tfrac{1}{2}\left(X_{i,j} - B_{i,j}\right)^2 \leq \alpha.
  \end{equation}
  This problem corresponds to formulation~\eqref{prob:primal3}. As with \autoref{ex:bpdn}, there are many dual methods that can generate dual feasible iterates $Y^{(t)}$ converging to the dual solution of~\eqref{eq:mc}, such as a dual bundle method~\cite{fan2019bundle}. The resulting primal-retrieval strategy for Step~3 of Algorithm~\ref{alg:primal_recover} can be implemented by executing the following steps:
  \begin{itemize}
      \item (Top-$k$ atoms) Compute the leading $k$ singular vectors of the matrix $M^*(Y^{(t)})$, given by $U_k^{(t)} \in \Re^{m \times k}$ and $V_k^{(t)} \in \Re^{n \times k}$ as defined by the SVD \eqref{eq-svd}.
      \item (Retrieve coefficients) Solve the reduced problem~\eqref{eq-spectral-reduced}, where in this case,
      \begin{equation} \label{eq:mc_pr}
          C^{(t)} \in \argmin{C\in\Re^{k\times k}} f_k(C),
          \text{with}
          f_k(C) \coloneqq \sum_{(i,j) \in \Omega}  \tfrac{1}{2}\left([U_k^{(t)}C(V_k^{(t)})^\intercal]_{i,j} - B_{i,j}\right)^2. 
      \end{equation}
      This least squares problem can be solved to within $\epsilon$-accuracy in $\BigOh(( k |\Omega| + (m+n)k + k^3 )\epsilon^{-0.5})$ iterations, for example, with the FISTA algorithm~\cite{beck2009fast}. Typically, $k \ll \min\{m,n\}$, and so we expect that this reduced problem is significantly cheaper to solve than the original problem~\eqref{eq:mc}.
      \item (Termination) Step~4 of Algorithm~\ref{alg:primal_recover} terminates when the value of the reduced objective satisifes the condition $f_k(C^{(t)}) \leq \alpha + \epsilon$, where $\epsilon$ is some pre-defined tolerance. In that case, the algorithm returns with the primal estimate constructed from the left and right singular vectors:
      \[X^{(t)} = U_k^{(t)}C^{(t)}(V_k^{(t)})^\intercal.\]
  \end{itemize}
  We describe numerical experiments for the low-rank matrix completion problem in \autoref{sec:lrmc}. 
\end{example}

\subsubsection{Iteration complexity}

In the polyhedral case, we were able to assert through \autoref{prop:polyhedral} that the optimal primal variable's atomic support could be identified in finite time. As we show here, however, finite-time identification is not possible for the spectral case. The following counterexample shows that the partial SVD of $M^*(Y)$, which we used in~\eqref{eq-svd}, is not able to give us a safe cover of the essential atoms in $\Escr\As(M^*(Y^*))$ even when this set is a singleton and $Y$ arbitrarily close to a dual solution $Y^*$.

\begin{example}[Limitation of Partial SVD]\label{ex:partial-svd}
	Consider the problem
\begin{align} \label{eq:example_P}
  \minimize{ X \in \mR^{n \times n} }\enspace \half \| X - B \|_F^2 \enspace\text{subject to}\enspace \| X \|_* \leq 1,
\end{align} 
where
\[
  B = U \Diag( 2, 0.1, \ldots, 0.1 ) V^T \text{and} U = V = \begin{bmatrix}
    \sqrt{1 - \epsilon} &~ 0 &  ~\ldots  & -\sqrt{ \epsilon } \\
    0 & ~1  &  ~\ldots &  \\
    \vdots &  &~\ddots & \\
    \sqrt{ \epsilon } & 0 & & \sqrt{1 - \epsilon}
    \end{bmatrix}_{n \times n}
\]
for some fixed $\epsilon \in (0,1)$. The dual problem is
\begin{align}
  \minimize{Y \in \mR^{n \times n} }\enspace \half \| Y - B \|_F^2 - \half \| B \|_F^2 + \|Y\|_2. \label{eq:example_D}
\end{align}
The solutions for the dual pair~\eqref{eq:example_P} and~\eqref{eq:example_D} are
\[
  X^* =  U \Diag( 1, 0, \ldots, 0 ) V^T \text{and} Y^* = B - X^* = U \Diag( 1, 0.1, \ldots, 0.1 ) V^T.
\]

In this pair of problems, the linear operator $M$ is simply the identity map, and the cone of essential atoms described by~\eqref{eq:cone_spectral} depends only on the dual variable $Y$.  Let $u_1$ and $v_1$, respectively, be the first columns of $U$ and $V$. Evidently, the support of $X^*$ coincides with the essential atoms of $Y^*$, and moreover, the support is a unique singleton. In other words,
\[
  \suppa(X^*) = \Escr\As(Y^*) = \{u_1 v_1^T\}.
\]
We construct the following dual feasible solution
\[
  \widehat{Y} = \Diag(1, 0.1, \ldots, 0.1).
\]
Because $\widehat Y$ is diagonal, its left and right singular vectors $\widehat{U}$ and $\widehat{V}$ are given by $\widehat{U} = \widehat{V} = I = [e_1, e_2, \ldots, e_n]$. Note also that the top singular vector $u_1 = [\sqrt{1 - \epsilon}, 0, \ldots, \sqrt{\epsilon}]^T$ lies in the span of the basis vectors $e_1$ and $e_n$ that constitute the top and bottom singular vectors of $\widehat{Y}$. Therefore, any top-$r$ SVD of $\widehat{Y}$, with $r < n$, cannot be used to recover exactly a primal solution $X^*$. Moreover, $\| \widehat{Y} - Y^* \|_F = \BigOh( \sqrt{\epsilon} )$ for any $\epsilon\in(0,1)$, which in effect implies that it's impossible to recover exactly the true solution even with an arbitrarily accurate dual approximation $\widehat{Y}$.
\end{example}

This last example motivates our study of the quality with which a partial SVD of a given feasible dual solution $M^*(Y)$ can be used to approximate the support $\Escr\As(M^*(Y^*))$. The next result measures the difference between $\suppa(x^*)$ and $\EssCone_{\Ascr, k}(M^*(Y))$ using one-sided Hausdorff distance.

\begin{proposition}[Error in truncated SVD] \label{thm:svd_approx_score}
  Let $Y$ be feasible for one of the dual problems~\Drobi\ for $i=1,2,3$.
Let $\{\sigma_j\}_{j=1}^{\min\{n,m\}}$ be the singular values satisfying 
$\sigma_1 \geq \dots \geq \sigma_{\min\{n,m\}}$, where we assume $\sigma_1 > \sigma_{k+1}$. Let $Z:=M^*(Y)$ and let $\EssCone_{\Ascr, k}(Z)$ denote the cone generated according to equation~\eqref{eq:cone_spectral}. Then 
  \[
    \dist( \suppa(X^*),\  \EssCone_{\Ascr, k}(Z)) \leq
    \dist( \Escr\As( Z, \epsilon_i ),\ \EssCone_{\Ascr, k}(Z)  ) \leq \sqrt{ 2 \min\left\{ \frac{\epsilon_i}{ \sigma_1 - \sigma_{k+1} }, 1 \right\} },
  \]
  where each $\epsilon_i$ is defined in \autoref{thm:p0} and the function 
  \[  
  \dist( \Ascr_1, \Ascr_2 ) \coloneqq \adjustlimits\sup_{a_1 \in \Ascr_1} \inf_{a_2 \in \Ascr_2 } \| a_1 - a_2 \|_F.
    \]
  is the one-sided Hausdorff distance between sets $\Ascr_1$ and $\Ascr_2$.
\end{proposition}

Oustry~\cite[Theorem~2.11]{Oustry00} developed a related result based on the two-sided Hausdorff distance. Directly applying Oustry's result to our context results in a bound on the order $ \mathcal{O}( \sqrt{ \epsilon/( \sigma_k - \sigma_{k+1} ) } ) $, which is looser than the bound shown in \autoref{thm:svd_approx_score} because $\sigma_1 \geq \sigma_{k} \geq \sigma_{k+1}$. 

Finally, we show the error bound for primary recovery. 
\begin{proposition} \label{prop:spectral}
     Let $X^*$ and $Y$, respectively, be primal optimal and dual feasible for one of the primal-dual pairs \Probi\ and \Drobi, for $i=1,2,3$.
Let $\{\sigma_j\}_{j=1}^{\min\{n,m\}}$ be the singular values satisfying 
$\sigma_1 \geq \dots \geq \sigma_{\min\{n,m\}}$, where we assume $\sigma_1 > \sigma_{k+1}$.  Let $\EssCone_{\Ascr, k}(M^*(Y))$ denote the cone generated according to equation~\eqref{eq:cone_spectral}. Let $\hat X$ be the solution recovered via~\eqref{eq:unsymm-recovered-soln}.
Then
\[
    f( B - M (\hat X )) \le f( B - M (X^*) ) + \BigOh\left( \sqrt{ \frac{\epsilon_i}{ \sigma_1 - \sigma_{k+1} } } \right),
\]
where each $\epsilon_i$ is defined in \autoref{thm:p0}.
\end{proposition}

\autoref{prop:spectral} characterizes the error bound for our primal-retrieval strategy when $\Ascr$ is spectral. Our next corollary shows that Algorithm~\ref{alg:primal_recover} can terminate in polynomial time with any tolerance $\epsilon > 0$. 

\begin{corollary} \label{coro:spectral}
  For one of the problems \Drobi, $i=1,2,3$, suppose that a dual oracle generates iterates $Y^{(t)}$ converging to optimal variable $Y^*$ with convergence rate
  \[d_i(Y^{(t)}) - d_i(Y^*) \in \Oscr\left( t^{-\alpha} \right)\]
  for some $\alpha > 0$. If the atomic set $\Ascr$ is spectral then Algorithm~\ref{alg:primal_recover} with $\epsilon>0$ will terminate in $\Oscr\left(\epsilon^{-4/\alpha}\right)$ iterations. 
\end{corollary}

\subsubsection{Non-centrosymmetry and constrained primal recovery} \label{sec:symmetric-matrices}

We now consider the case in which the atomic set $\Ascr$ given by \eqref{eq:symmetric}, which is not centrosymmetric. As we show below, the corresponding primal recovery problem is constrained.

Fix a feasible dual variable $Y$ and define the eigenvalue decomposition for its product with the adjoint of $M$ by
\begin{equation}
   M^*(Y) = 
\begin{bmatrix}
    V_k & V_{-k}
\end{bmatrix}
\begin{bmatrix}
    \Sigma_k &          \\ 
             &  \Sigma_{-k}
\end{bmatrix}
\begin{bmatrix}
    V_k\T \\ 
    V_{-k}\T
\end{bmatrix},
\end{equation}
where $V_k$ and the diagonal matrix $\Sigma_k$, respectively, contain the top-$k$ eigenvectors and eigenvalues of $M^*(Y)$, and $V_{-k}$ and the diagonal matrix $\Sigma_{-k}$, respectively, contain the remaining eigenvectors and eigenvalues. Then the reduced atomic set by $V_k$ can be expressed as 
\[
  \Ascr_k = \{ vv\T | v\in\range(V_k),\ \|v\|_2=1\} \subset \Ascr.
\]
The convex cone of essential atoms generated from the reduced atomic set $\Ascr_k$ is given by
\begin{equation} \label{eq:cone_spectral2}
    \EssCone_{\Ascr, k}(M^*(Y)) \coloneqq \cone(\Ascr_k) = \{V_k C V_k^T | C \in \Re^{k\times k},\ C \succeq 0\}.
\end{equation}
The recovery problem \eqref{eq-spectral-reduced} then becomes constrained, i.e., 
\[
  \hat C \in \argmin{C\in\Re^{k\times k},\ C \succeq 0} f_k(C).
\] 

\section{Numerical experiments}

We conduct several numerical experiments on both synthetic and real-world datasets to empirically verify the effectiveness of our proposed primal-retrieval strategy. In \autoref{sec:bpdn}, we conduct experiments on the basis pursuit denoising problem (\autoref{ex:bpdn}), which shows the performance of our strategy on polyhedral atomic set. In \autoref{sec:lrmc}, we apply our primal-retrieval technique to the low-rank matrix completion problem (\autoref{ex:mc}) and test the effectiveness of our proposed method on the spectral atomic set. In \autoref{sec:rpca}, we conduct experiment on 
a image preprocessing problem, where the atomic set $\Ascr$ is the sum of a polyhedral atomic set and a spectral atomic set. This shows that our strategy can be applied to more complicated cases. The optimization problems are solved using the Julia package \texttt{AtomicOpt.jl} that will be introduced in \autoref{ch:App-AtomicOpt}. All the experiments are conducted on a Linux server with 8 CPUs and 64Gb memory.

\subsection{Basis pursuit denoise} \label{sec:bpdn}

This section summarizes a series of numerical experiments in which we apply our primal-retrieval strategy to basis pursuit denoising problems (\autoref{ex:bpdn}). The experiments include a selection of five relevant problems from the Sparco collection~\cite{BergFrieHennHerrSaabYilm:2008} of test problems. The chosen problems are all real-valued and suited to one-norm regularization. Each problem in the collection includes a linear operator $M:\Re^n \to \Re^m$ and a right-hand-side vector $b \in \Re^m$. \autoref{tab:sparco_info} summarizes the selected problems. We compare the results with SPGL1~\cite{BergFriedlander:2008}. In all problems, we set $\alpha = 10^{-3}\cdot\|b\|$. The results are shown in \autoref{tab:bpdn} where \texttt{nMat} denotes the total number of matrix-vector products with $M$ or $M^*$. 
As we can observe from \autoref{tab:bpdn}, the level-set algorithm equipped with our primal-retrieval technique can obtain an $\epsilon$-feasible solution within a small number of iterations, which is consistent with the finite-time identification property (\autoref{prop:polyhedral}). Moreover, we can also observe that level-set with primal-retrieval can converge faster than SPGL1 with its default stopping criterion. This suggests that our primal-retrieval technique is not only a memory-efficient method to obtain primal variables with provable error bounds, but also a practical technique that allow the optimization algorithm to stop early.

\begin{table}[t]
    \centering
    \begin{tabular}{llllll}
        \toprule
        Problem             &   ID &    $m$ &   $n$ &   $\|b\|$ &   $M$\\
        \texttt{blocksig}   &   2  &    1024&   1024&   7.9e+1  &   wavelet\\
        \texttt{cosspike}   &   3  &    1024&   2048&   1.0e+2  &   DCT\\
        \texttt{gcosspike}  &   5  &    300 &   2048&   8.1e+1  &   Gaussian ensemble + DCT\\
        \texttt{sgnspike}   &   7  &    600 &   2560&   2.2e+0  &   Gaussian ensemble\\
        \texttt{spiketrn}   &   903&    1024&   1024&   5.7e+1  &   1D convolution\\
        \bottomrule
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{The Sparco test problems used.}
    \label{tab:sparco_info}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        Problem             &   \texttt{nnz}(x) &    \texttt{nMat}(SPGL1) & \texttt{nMat}(level-set+PR)\\
        \texttt{blocksig}   &   71              &     22                  &   5\\
        \texttt{cosspike}   &   113             &     77                  &   71\\
        \texttt{gcosspike}  &   113             &    434                  &   141\\
        \texttt{sgnspike}   &   20              &    44                   &   21\\
        \texttt{spiketrn}   &   35              &    4761                 &   1888\\
        \bottomrule
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{Basis pursuit denoise comparisons.}
    \label{tab:bpdn}
\end{table}

\subsection{Low-rank matrix completion} \label{sec:lrmc}

We conduct the similar experiment as in \cite{candes2010matrix}. We retrieved from the website of National Centers for Environmental Information\footnote{\url{https://www.ncei.noaa.gov}} a matrix $X \in \Re^{6798 \times 366}$ whose entries are daily average temperatures at 6798 different weather stations throughout the world in year 2020. The temperature matrix $X$ is approximately low rank in the sense that $\|X - X_5\|_F / \|X\|_F \approx 24\%$, where $X_5$ is the matrix created by truncating the SVD after the top 5 singular values. 

To test the performance of our matrix completion algorithm, we subsampled 50\% of $X$ and then recovered an estimate, $\hat X$.  The solution gives a relative error of $\|X - \hat X\|_F / \|X\|_F \approx 30\%$. The result is shown in \autoref{fig:matrix_completion}.  As we can see from \autoref{fig:matrix_completion}, the recovery error exhibits a positive correlation with the duality gap, both the duality gap and the recovery gap decrease as the number of iteration increase. The observation in this experiment is consistent with our theory (\autoref{prop:spectral}).

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{./figures/matrix_completion.pdf}
    \captionsetup{justification=centering}
    \caption{Matrix completion experiment.}
    \label{fig:matrix_completion}
\end{figure}

\subsection{Robust Principal Component Analysis} \label{sec:rpca}

\begin{figure}[t] \label{fig:rpca}
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=\linewidth]{./figures/face.png}
      \captionsetup{justification=centering}
      \caption{Original faces.}
      \label{fig:face}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=\linewidth]{./figures/faceDeshadow.png}
      \captionsetup{justification=centering}
      \caption{Shadow-removed faces.}
      \label{fig:face_shadow}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{Robust principal component analysis experiment.}
\end{figure}

In this section, we show that our primal-retrieval strategy can be applied to more complicated atomic sets besides polyhedral and spectral. We conduct the the similar experiment as in \cite{candes2011robust}. Face recognition algorithms are sensitive to shadows on faces. In order to obtain the best possible performance for these algorithms, it is desired to remove illumination variations and shadows on the face images. We obtained face images from the Yale B face database~\cite{georghiades2001few}. We show the original faces in \autoref{fig:face}, where each face image was of size $192\times 168$ with 64 different lighting conditions. The images were then reshaped into a matrix $M \in \Re^{32256 \times 64}$. Because of the similarity between faces and the sparse structure of the shadow, the matrix $M$ can be approximately decomposed into two components, i.e., 
\[M \approx L + S,\]
where $L$ is a low-rank matrix corresponding to the clean faces and $S$ is sparse matrix corresponding to the shadows. Based on the work by Fan et al.~\cite{fan2020polar}, we know that such decomposition can be obtained via solving the following convex optimization problem:
\begin{equation} \label{eq:rpca}
    \min_{L, S} \enspace \max\{\|L\|_*, \lambda \|S\|_1\}  \enspace \st  \enspace  \|L + S - M\| \leq \alpha.
\end{equation}
By \cite[Proposition~7.3]{fan2019alignment}, we know that \eqref{eq:rpca} is equivalent to 
\begin{equation} \label{eq:rpca2}
    \min_{x} \enspace \gauge\As(x)  \enspace \st  \enspace  \|x - M\| \leq \alpha,
\end{equation}
where $\Ascr = \lambda \Ascr_1 + \Ascr_2$, $\Ascr_1 = \{uv^T \mid u \in \mR^m,\ v \in \mR^n,\ \|u\|_2=\|v\|_2 = 1\}$ and $\Ascr_2 = \{\pm e_ie_j^T \mid i \in [m], j \in [n]\}$. The recovered low-rank component is shown in \autoref{fig:face_shadow}. As we can see from the figure, most of the shadow are successfully removed. This experiment suggests that our primal-retrieval technique can potentially be used for more complex atomic set and allow the underlying the dual-algorithm to produce satisfactory result within a reasonable number of iterations. 





\section{Proofs}

\subsection{Derivation of duals}\label{app:duals}

We derive the dual problems~\eqref{prob:dual1},~\eqref{prob:dual2} and~\eqref{prob:dual3} using the Fenchel\textendash Rockafellar duality framework. We use the following result.
\begin{theorem}[\protect{\cite[Corollary~31.2.1]{rockafellar1970convex}}]\label{thm-fenchel}
  Let $f_1:\Re^n\to\Re$ and $f_2:\Re^m\to\Re$ be two closed proper convex functions and let $M$ be a linear operator from $\Re^n$ to $\Re^m$, then
  \[\inf_{x \in \Re^n} f_1(x) + f_2(Mx) = \inf_{y \in \Re^m} f_1^*(M^*y) + f_2^*(-y).\]
  If there exist $x$ in the interior of $\dom f_1$ such that $Mx$ in the interior of $\dom f_2$, then strong duality holds, namely both infima are attained. 
\end{theorem}
We also need a result that describes the relationship between gauge, support, and indicator functions. 
\begin{proposition}[\protect{\cite[Proposition~3.2]{fan2019alignment}}] \label{prop-conjugate-gauge}
  Let $C\subset\Re^n$ be a closed convex set that contains the origin. Then 
  \[\gauge_C = \sigma_{C^\circ}=\delta_{C^\circ}^*.\]
\end{proposition}

For problem~\eqref{prob:primal1}, let
\[
  f_1 := \lambda\gauge\As \enspace\text{and}\enspace f_2 := f(b - \cdot)
\]
By the properties of conjugate functions and~\autoref{prop-conjugate-gauge}, we obtain 
\[
  f_1^* =  \delta_{(\frac{1}{\lambda}\Ascr)^\circ} = \delta_{\{x\mid \sigma\As(x)\leq\lambda\}} \enspace\text{and}\enspace f_2^* = \ip{b}{\cdot} + f^*(-\cdot).
\]
Then by~\autoref{thm-fenchel}, we can get the dual problem for~\eqref{prob:primal1} as
\[\minimize{y\in\Re^m} f^*(y) - \ip{b}{y} \enspace\text{subject to}\enspace \sigma\As(M^*y)\leq\lambda.\]

For~\eqref{prob:primal2},
\[f_1 = \delta_{\gauge\As\leq\tau} = \delta_{\tau\Ascr} \enspace\text{and}\enspace f_2 = f(b - \cdot).
\]
By the properties of conjugate functions and~\autoref{prop-conjugate-gauge}, we obtain 
\[f_1^* = \sigma_{\tau\Ascr} = \tau\sigma\As \enspace\text{and}\enspace f_2^* = \ip{b}{\cdot} + f^*(-\cdot).
\]
Then by~\autoref{thm-fenchel}, it follows that the dual problem for~\eqref{prob:primal2} is 
\[\minimize{y\in\Re^m} f^*(y) - \ip{b}{y} + \tau\sigma\As(M^*y).\]

For~\eqref{prob:primal3},
\[
  f_1 = \gauge\As \enspace\text{and}\enspace f_2 = \delta_{\{x\mid f(b - x)\leq\alpha\}}.
\]
By the properties of conjugate functions and~\autoref{prop-conjugate-gauge}, we can get that 
\[
  f_1^* = \delta_{\{x\mid \sigma\As(x)\leq1\}} \enspace\text{and}\enspace f_2^* = \sigma_{\{f(b - x)\leq\alpha\}}.\]
Then by~\cite[Example~E.2.5.3]{hiriart-urruty01}, we know that the support function of the sublevel set is 
\[f_2^* = \sigma_{\{x\mid f(b - x)\leq\alpha\}} = \min_{\beta > 0} \beta\left(f^*\left(-\frac{\cdot}{\beta}\right) + \alpha\right) + \ip{b}{\cdot}.\]
Finally, by~\autoref{thm-fenchel}, we can get the dual problem for~\eqref{prob:primal3} as
\[\minimize{y\in\Re^m,\ \beta > 0} \beta \left( f^*\left( \frac{y}{\beta} \right) + \alpha \right) - \ip{b}{y} \enspace \text{subject to}\enspace \sigma\As(M^* y) \leq 1.\]


\subsection{Proof of \autoref{thm:p0}} \label{app:main_proof}

The proof of this Theorem relies on the duality between smoothness and strong convexity.
\begin{lemma}[\protect{\cite[Theorem~6]{kakade2009duality}}] \label{lemma:conjugate}
   If $f$ is $L$-smooth, then $f^*$ is $\frac{1}{L}$-strongly convex.
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:p0}]
    \begin{itemize}
      \item[a)] Let $y^*$ denote the optimal dual variable for~\eqref{prob:dual1}. First, we show that $\|y - y^*\|$ can be bounded by the duality gap. Let $g(y) = f^*(y) - \ip{b}{y}$. By~\autoref{lemma:conjugate}, $f^*$ is $\frac{1}{L}$-strongly convex, and it follows that $g$ is also $\frac{1}{L}$-strongly convex. By the definition of strong convexity, 
      \[\forall s \in \partial g(y^*), \enspace g(y) \geq g(y^*) + \ip{s}{y- y^*} + \frac{1}{2L}\|y - y^*\|^2.
      \]
      Optimality requires that 
      \[\exists s \in \partial g(y^*), \enspace \ip{s}{y- y^*} \geq 0 \quad \forall y \enspace\text{s.t.}\enspace \sigma\As(M^*y)\leq\lambda.\]
      Therefore, by reordering the inequality,
      \begin{align}
         \|y - y^*\| & \leq \sqrt{2L(g(y) - g(y^*))}
         \quad \forall x \in \Re^n.
      \end{align} 
    
      Next, we show that $\Escr\As(M^*y^*) \subseteq \Escr\As(M^*y, \epsilon_1)$. For any $a \in \Escr\As(M^*y^*)$, 
      \begin{align*}
        \ip{a}{M^*y} &= \sigma\As(M^*y^*) + \ip{Ma}{y - y^*}
        \\&\geq \sigma\As(M^*y^*) - \left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&\geq \sigma\As(M^*y^*) - \left(\max\limits_{a \in \Ascr}\|Ma\|\right)\sqrt{2L(g(y) - g(y^*))}
        \\&\geq \sigma\As(M^*y) - \epsilon_1,
      \end{align*}
      where the last inequality follows from the fact that $\sigma\As(M^*y^*) = \lambda$ and $y$ is feasible for \eqref{prob:dual3}. 
    
      \item[b)] Let $y^*$ denote the optimal dual variable for~\eqref{prob:dual2}. First, we show that $\|y - y^*\|$ can be bounded by the duality gap. Let $g(y) = f^*(y) - \ip{b}{y} + \tau\sigma\As(M^*y)$. By~\autoref{lemma:conjugate}, $f^*$ is $\frac{1}{L}$-strongly convex, and it follows that $g$ is also $\frac{1}{L}$-strongly convex. 
      By the definition of strongly convex, 
      \[\forall s \in \partial g(y^*), \enspace g(y) \geq g(y^*) + \ip{s}{y- y^*} + \frac{1}{2L}\|y - y^*\|^2.\]
      By optimality, $0 \in \partial g(y^*)$. Reorder the inequality to deduce that
      \[\|y - y^*\|_2 \leq \sqrt{2L(g(y) - g(y^*))}.\]
    
      Next, we show that $\Escr\As(M^*y^*) \subseteq \Escr\As(M^*y, \epsilon_2)$. For any $a \in \Escr\As(M^*y^*)$, 
      \begin{align*}
        \ip{a}{M^*y} &\geq \sigma\As(M^*y^*) - \left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&= \sigma\As(M^*y) - \left(\sigma\As(M^*y) - \sigma\As(M^*y^*)\right) - \left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&\geq \sigma\As(M^*y) - 2\left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&\geq \sigma\As(M^*y) - \epsilon_2.
      \end{align*}
    
      \item[c)] Let $(y^*, \beta^*)$ denote the optimal dual variables for~\eqref{prob:dual3}. First, we show that $\|y - y^*\|$ can be bounded by the duality gap. Let 
      \[g(y) = \beta^*f^* \left(\frac{y}{\beta^*} \right) + \beta^*\alpha - \ip{b}{y}.\] 
      By~\autoref{lemma:conjugate}, $f^*$ is $\frac{1}{L}$-strongly convex, and it's not hard to check that $g$ is $\frac{1}{\beta^*L}$-strongly convex. By the definition of strongly convex,
      \[\forall s \in \partial g(y^*), \enspace g(y) \geq g(y^*) + \ip{s}{y- y^*} + \frac{1}{2\beta^*L}\|y - y^*\|^2.\]
      By optimality,
      \[\exists s \in \partial g(y^*), \enspace\ip{s}{y- y^*} \geq 0 \quad \forall y \enspace\text{s.t.}\enspace \sigma\As(M^*y)\leq1.\]
      Reorder the inequality to deduce that 
      \[\|y - y^*\|_2 \leq \sqrt{2\beta^*L(g(y) - g(y^*))}.\]
    %   \begin{align}
    %      \|y - y^*\|_2 & \leq \sqrt{2\beta^*L(g(y) - g(y^*))} \nonumber \\
    %      & \leq \sqrt{2\beta^*L \left(p_3(x) + d_3(y, \beta^*)\right) } \enspace \forall x \in \Re^n \text{s.t.} f(b - Mx)\leq\alpha\nonumber
    %      \\&\leq \sqrt{2\overline{\beta}L \left(p_3(x) + d_3(y, \beta^*)\right) } \enspace \forall x \in \Re^n \text{s.t.} f(b - Mx)\leq\alpha.
    %   \end{align} 
    
      Since $\beta^*$ is unknown to us, we will then get an upper bound for $d_3(y, \beta^*)$. Fix $y$, let $h(\beta) = d_3(y, \beta)$. By the property of perspective function, we know that $h$ is convex. Then it follows that 
      \[d_3(y, \beta^*) \leq \max\{d_3(y, \underline{\beta}), d_3(y, \overline{\beta})\}.\]
      Therefore,
      \[ \|y - y^*\|_2 \leq \sqrt{2\overline{\beta}L \left( \max\{d_3(y, \underline{\beta}), d_3(y, \overline{\beta})\} - d_3(y^*)\right) } .\]
    
      Finally, we show that $\Escr\As(M^*y^*) \subseteq \Escr\As(M^*y, \epsilon_3)$. For any $a \in \Escr\As(M^*y^*)$, 
      \begin{align*}
        \ip{a}{M^*y} &\geq \sigma\As(M^*y^*) - \left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&= \sigma\As(M^*y) - \left(\sigma\As(M^*y) - \sigma\As(M^*y^*)\right) - \left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&\geq \sigma\As(M^*y) - 2\left(\max\limits_{a \in \Ascr}\|Ma\|\right)\|y - y^*\|
        \\&\geq \sigma\As(M^*y) - \epsilon_3.
      \end{align*}
    \end{itemize}
\end{proof}


\subsection{Upper and lower bounds for \texorpdfstring{$\beta^*$}{} }
\label{app:bounds}
First, we consider~\eqref{prob:dual3}. Let $w = y/\beta$, then~\eqref{prob:dual3} can be equivalently expressed as 
\[\minimize{w} \inf_{\beta>0} \beta(f^*(w) - \ip{b}{w} + \alpha) \enspace\text{subject to}\enspace \sigma\As(M^* w) \leq \beta.\]
Fix $\beta = \beta^*$, then~\eqref{prob:dual3} can be expressed as 
\begin{equation} \label{prob:dual3equiv}
  \minimize{w} f^*(w) - \ip{b}{w} \enspace\text{subject to}\enspace \sigma\As(M^* w) \leq \beta^*.
\end{equation}
Now compare~\eqref{prob:dual3equiv} with~\eqref{prob:dual1} to conclude that they are equivalent when $\lambda = \beta^*$. It thus follows that~\eqref{prob:primal3} is equivalent to  
\begin{equation} \label{prob:p1beta}
    \minimize{x} f(b - Mx) + \beta^*\gauge\As(x). 
\end{equation}   

Next, consider using the level-set method~\cite{aravkin2016levelset} with bisection to solve~\eqref{prob:primal3}. There exists $\tau^* > 0$ such that~\eqref{prob:primal3} is equivalent to
\begin{equation} \label{prob:p2tau}
    \minimize{x} f(b - Mx) \enspace\text{subject to}\enspace \gauge\As(x)\leq\tau^*. 
\end{equation}
With the level-set method, we are able to get $(x_1, \tau_1)$ and $(x_2, \tau_2)$ such that $\tau_1 \leq \tau^* \leq \tau_2$ and $x_i$ is the optimum for 
\begin{equation} \label{prob:p2taui}
    \minimize{x} f(b - Mx) \enspace\text{subject to}\enspace \gauge\As(x)\leq\tau_i, 
\end{equation}
for $i = 1, 2$. Then there exits $\beta_1$ and $\beta_2$ such that $\beta_1 \geq \beta^* \geq \beta_2$ and $x_i$ is optimal for 
\begin{equation} \label{prob:p1betai}
    \minimize{x} f(b - Mx) + \beta_i\gauge\As(x),
\end{equation}
for $i = 1, 2$.

Finally, by~\cite[Theorem~5.1]{fan2019alignment} we can conclude that 
\[\beta_i = \sigma\As(M^*\nabla f(b - Mx_i)) \text{for} i = 1,2.\]
Therefore, we can get upper and lower bounds for $\beta^*$ via level-set method with bisection. Moreover, by strong duality and convergence of the bisection method, the gap between $\beta_1$ and $\beta_2$ will converge to zero. 

\subsection{Proof of Proposition~\ref{prop:polyhedral}}
\label{app:prop_proof}
\begin{proof}
  First, we show that for any $y_i$ such that $\| y_i - y_i^* \| \leq \frac{\delta}{4\| M \|_\Ascr}$, the condition 
  \[\Face\As( M^* y_i^*) \subseteq \texttt{EssCone}(\Ascr, M, y_i, k)\]
  holds. By \autoref{ass:blanket} and the definition of $\delta$-nondegeneracy, we know that 
  \begin{equation} \label{eq:Aprime}
  \begin{split}
    |  \Face\As( M^* y_i^*) | &= k \text{and} \\
    \langle Ma, y_i^* \rangle &\leq \sigma_{\Ascr}( M^* y_i^* ) - \delta \quad \forall a \notin \Face\As( M^* y_i^*).
  \end{split}
  \end{equation}
  For any $a \in \Face\As( M^* y_i^*)$, we have
  \begin{align*}
      & \langle a, M^* y_i \rangle \\
      \geq~& \langle a, M^* y_i^* \rangle - | \langle M a, y_i^* - y_i \rangle | \\
      \geq~& \sigma_{\Ascr}( M^* y_i^* ) - \| M \|_\Ascr \frac{\delta}{4\|M\|_{\Ascr}} \\
      \geq~& \sigma_{\Ascr}( M^* y_i^* ) - \frac{\delta}{4}.
  \end{align*}
  For any $a' \notin \Face\As( M^* y_i^*)$, we have 
  \begin{align*}
      & \langle a', M^* y_i \rangle \\
      \leq~ & \langle a', M^* y_i^* \rangle + | \langle M a', y_i^* - y_i \rangle | \\
      \leq~ & \langle a', M^* y_i^* \rangle + \frac{\delta}{4} \\
      \leq~ & \sigma_{\Ascr}( M^* y_i^* ) - \delta + \frac{\delta}{4} \quad \text{(By \eqref{eq:Aprime})} \\
      =~& \sigma_{\Ascr}( M^* y_i^* ) - \frac{3\delta}{4}.
  \end{align*}
    Therefore, 
    \begin{align*}
        \langle a, M^* y_i \rangle > \langle a', M^* y_i \rangle  \quad \forall a \in \Face\As( M^* y_i^*) \text{and} a' \notin \Face\As( M^* y_i^*).
    \end{align*}
    Notice that $\texttt{EssCone}(\Ascr, M, y_i, k)$ contains only the atoms that corresponds to the $k$ largest $\langle a, M^* y_i \rangle$. Therefore $\Face\As( M^* y_i^*) \subseteq \texttt{EssCone}(\Ascr, M, y_i, k)$.
    
    By the assumption $y_i^{(t)} \to y_i^*$. For $i \in \{1,2,3\}$, we know there exist $T_i > 0$ such that $\| y_i^{(t)} - y_i^* \| < \frac{\delta}{4\| M \|_\Ascr}$ for all $t > T$. Therefore 
    \[\Face\As( M^* y_i^*) \subseteq \texttt{EssCone}(\Ascr, M, y_i^{(t)}, k)~~\forall t > T\] 
    and we complete the proof.
\end{proof}

\subsection{Proof for \autoref{thm:svd_approx_score}}
\begin{proof}
  By the definition of $\rho(\cdot, \cdot)$, it follows that
  \[
    \rho(A, C) \leq \rho(B, C) \quad \forall A, B, C \subseteq \mR^{n \times m} ~\text{such that}~ A \subseteq B.
  \]
  We know that $ \Face\As( M^* y^* ) \subseteq  \Face\As( M^* y, \epsilon )$, then obviously we have 
  \[
    \rho( \Face\As( M^* y^* ), \widehat{\Ascr}  ) \leq \rho( \Face\As( M^* y, \epsilon ), \widehat{\Ascr}  ).
  \]
  For any $\Ascr_1, \Ascr_2 \subseteq \Ascr$,
  \begin{align}
    \rho( \Ascr_1, \Ascr_2 ) = \sqrt{ \adjustlimits\sup_{a_1 \in \Ascr_1} \inf_{a_2 \in \Ascr_2} \| a_1 - a_2 \|_F^2 } = \sqrt{ 2 - 2 \left( \adjustlimits \inf_{a_1 \in \Ascr_1} \sup_{a_2 \in \Ascr_2} \ip{a_1}{a_2} \right) }, \label{eq:norm2ip}
  \end{align}
  where the second equality holds since $\| a_1 \|_F = \| a_2 \|_F = 1$ by the definition of $\Ascr$. Define $\Ascr_1 = \Face\As( M^* y, \epsilon )$ and $\Ascr_2 = \widehat{\Ascr} = \{ U_r p q^T V_r^T | \|p\|_2 = \|q\|_2 = 1 \}$, where $U_r, V_r$ are the top-$r$ singular vectors of $M^*y$. Let $k \coloneqq \min\{n,m\}$, $\Cscr_1 = \{(p, q) \mid \sum_{i=1}^{k} \sigma_i p_i q_i \geq \sigma_1 - \epsilon,~ \|p\|_2 = \|q\|_2 = 1, ~p,q \in \mR^{ k }\}$ and $\Cscr_2 = \{(\hat{p}, \hat{q}) \mid \|\hat{p}\|_2 = \|\hat{q}\|_2 = 1, ~\hat{p},\hat{q} \in \mR^{ k }\}$, then
  \begin{align}
    \rho( \Ascr_1, \Ascr_2  ) & = \sqrt{ 2 - 2 \left( \min_{p, q \in \Cscr_1} \max_{ \hat{p}, \hat{q} \in \Cscr_2} \ip{ Up q^T V^T }{ U_r \hat{p} \hat{q}^T V_r^T } \right) } \nonumber \\
    & = \sqrt{ 2 - 2 \left( \min_{p, q \in \Cscr_1} \max_{ \hat{p}, \hat{q} \in \Cscr_2} \left( \sum_{i=1}^r p_i \hat{p}_i \right) \left( \sum_{i=1}^r q_i \hat{q}_i \right) \right) } \nonumber \\
    & = \sqrt{ 2 - 2 \left(\min_{p, q \in \Cscr_1} \| p_{1:r} \|_2 \| q_{1:r} \|_2 \right) } \label{eq:rho_exp} 
    % \text{subject to} & \sum_{i=1}^{k} \sigma_i p_i q_i \geq \sigma_1 - \epsilon,~ \|p\|_2 = \|q\|_2 = 1, ~p,q \in \mR^{ k }, \nonumber \\
  \end{align}
  Now we consider the subproblem in \eqref{eq:rho_exp}: 
  \begin{align}
    \min_{p,q}\enspace & \| p_{1:r} \|_2 \| q_{1:r} \|_2 \label{eq:reduced_p1} \tag{P$_1$} \\
    \text{subject to} & \sum_{i=1}^{k} \sigma_i p_i q_i \geq \sigma_1 - \epsilon,~ \|p\|_2 = \|q\|_2 = 1, ~p,q \in \mR^{ k }. \nonumber
  \end{align}
  If $p^*$ and $q^*$ is a solution of the problem \eqref{eq:reduced_p1}, then it's easy to verify that 
  \begin{align*}
    & \tilde{p} = \left[ \| p^*_{1:r} \|_2, 0,\ldots, \|p^*_{r+1:k}\|_2, 0, \ldots, 0 \right] \\ 
    \text{and} ~& \tilde{q} = \left[ \| q^*_{1:r} \|_2, 0,\ldots, \|q^*_{r+1:k}\|_2, 0, \ldots, 0 \right]
  \end{align*}
  is also a valid solution. Therefore there must exist solution $p^*, q^*$ such that $p_i = q_i = 0~\forall i \notin \{1, r+1\}$, that is only $p^*_1, q^*_1$ and $p^*_{r+1}$ and $q^*_{r+1}$ are greater or equal than 0. This allow us to further reduce the problem to
  \begin{align*}
    \min_{ p_1, q_1, p_{r+1}, q_{r+1} } &~  p_1 q_1 \\
    \text{subject to} & \sigma_1 p_1 q_1 + \sigma_{r+1} p_{r+1} q_{r+1} \geq \sigma_1 - \epsilon, \\
    & p_1^2 + p_{r+1}^2 = q_1^2 + q_{r+1}^2 = 1, ~p_1, q_1, p_{r+1}, q_{r+1} \geq 0. 
  \end{align*}
  It's easy to verify that when $\sigma_1 - \sigma_{r+1} \geq \epsilon$, the above problem attains solution at 
  \[
    p_1 = q_1 = \sqrt{\frac{\sigma_1 - \sigma_{r+1} - \epsilon}{ \sigma_1 - \sigma_{r+1} } } \text{and} p_{r+1} = q_{r+1} = \sqrt{1 - p_1^2}.
  \]
  When $\sigma_1 - \sigma_{r+1} < \epsilon$, the solution is simply $p_1 = q_1 = 0, p_{r+1}= q_{r+1} = 1$.
  Therefore the optimal value of \eqref{eq:reduced_p1} is $\max\{1 - \epsilon / ( \sigma_1 - \sigma_{r+1} ), 0\}$, plug this into \eqref{eq:rho_exp} and the proof is finished. 
\end{proof}

\begin{proposition}[Hausdorff error bound] \label{prop:dist_to_sol}
    Given $\widehat{\Ascr} \subseteq \Ascr$ such that , there exists $ x \in \cone ( \widehat{\Ascr} ) $ such that 
    \begin{equation}
            \| x - x^* \|_F \leq \dist( \suppa(x^*), \widehat{\Ascr} )\cdot \sqrt{| \suppa(x^*) |} \cdot \| x^* \|_F.  \label{eq:dist_to_sol}
    \end{equation}
\end{proposition}

\subsection{Proof of \autoref{prop:spectral}}
\begin{proof}
  By \autoref{prop:dist_to_sol}, we know that there exist $\tilde{x}$ satisfies \eqref{eq:dist_to_sol}.
  Then by the $L$-smoothness of $f$,
  \begin{align}
      f( b - M \tilde{x} ) &~\leq~ f( b - M x^* ) + \langle \nabla f( b - M x^* ), M( x^* - \tilde{x} ) \rangle + \frac{L}{2} \| M( x^* - \tilde{x} ) \|^2  \nonumber \\
      &~\leq~ f( b - M x^* ) + \| \nabla f( b - M x^* ) \| \| M( x^* - \tilde{x} ) \| + \frac{L}{2} \| M( x^* - \tilde{x} ) \|^2. \label{eq:errorBoundIntermediate1}
  \end{align}
  By the smoothness and convexity of $f$, we further have
  \begin{align*}
      \| \nabla f( b - M x^* ) - \nabla f(0) \|^2 \leq 2L ( f(b - M x^*) - f(0) ).
  \end{align*}
  Note that $f(0)$ and $\nabla f(0) = 0$, the above reduces to $\| \nabla f( b - M x^* ) \| \leq \sqrt{2 L \alpha}$. Combining with \eqref{eq:errorBoundIntermediate1}, we obtain
  \begin{align*}
      f( b - M \tilde{x} ) &~\leq~ f( b - M x^* ) + \sqrt{2 L \alpha} \| M( x^* - \tilde{x} ) \| + \frac{L}{2} \| M( x^* - \tilde{x} ) \|^2 \\
      &~\leq~ f( b - M x^* ) + \sqrt{2 L \alpha} \| M \| \rho( \suppa(x^*), \widehat{\Ascr} )\cdot \sqrt{| \suppa(x^*) |} \cdot \| x^* \|_F  \\
      &\qquad + \frac{L}{2} \rho( \suppa(x^*), \widehat{\Ascr} )^2 | \suppa(x^*) | \| x^* \|_F^2.
  \end{align*}
\end{proof}
























